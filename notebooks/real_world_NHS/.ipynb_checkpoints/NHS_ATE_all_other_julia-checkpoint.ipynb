{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CSV\n",
    "using Statistics\n",
    "using Distributions\n",
    "using Optim\n",
    "using ForwardDiff\n",
    "using Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using IJulia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct VertexLabels\n",
    "  u1::Vector{Int}  # Latent variables, parents of colliders Z1 and outcome Y\n",
    "  u2::Vector{Int}  # Latent variables, parents of colliders Z1 and treatment X\n",
    "  w::Vector{Int}       # Instrument\n",
    "  x::Vector{Int}       # Treatment\n",
    "  y::Vector{Int}       # Outcome\n",
    "  z1::Vector{Int}  # Colliders between X and Y\n",
    "  z3::Vector{Int}  # Confounders between X and Y\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# graph definitions\n",
    "w = collect(1:1)\n",
    "x = collect(2:2)\n",
    "y = collect(3:3)\n",
    "u1 = collect(4:8)\n",
    "u2 = collect(9:13)\n",
    "z1 = collect(14:18)\n",
    "z3 = collect(19:25)\n",
    "vertex_labels = VertexLabels(u1, u2, w, x, y, z1, z3)\n",
    "num_vertices = 25\n",
    "n = 90000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper function from app_linear.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "choose_lambdas (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function choose_lambdas(lambda_twos, lambda1, seeds, dat, vertex_labels, max_iter, lrs, corr_boost, path_to_file)\n",
    "  initial_lambda1 = lambda1\n",
    "  corr_pxs = Dict() \n",
    "  hypo_fail_all_lambdas = Dict()\n",
    "  num_Z = length(vertex_labels.z1)+length(vertex_labels.z3)\n",
    "#   ATE_results_test = nothing\n",
    "\n",
    "\n",
    "  w, x, y = vertex_labels.w, vertex_labels.x, vertex_labels.y\n",
    "  Z = [vertex_labels.z1; vertex_labels.z3]\n",
    "  best_Z = vertex_labels.z3\n",
    "\n",
    "  split = trunc(Int,(size(dat)[1]/3))\n",
    "  dat_train = dat[1:split,:]\n",
    "  dat_valid = dat[split+1:split*2,:]\n",
    "  dat_test = dat[(split*2)+1:end,:]\n",
    "  # Sigma train construction\n",
    "  Sigma_hat = cov(dat_train)\n",
    " for seed in seeds\n",
    "    Random.seed!(seed);\n",
    "  for lambda2 in lambda_twos\n",
    "    println(\"lambda2: \", lambda2)\n",
    "    for lr in lrs\n",
    "      hypo_fail = Dict()\n",
    "      println(\"lr: \", lr)\n",
    "      reject_null_hypothesis = false\n",
    "      lambda1 = initial_lambda1\n",
    "      bonferonni_correction = 1.\n",
    "      while (!reject_null_hypothesis)\n",
    "\n",
    "        # learn theta on training set\n",
    "        theta_hat, corr_px_theta_hat, corr_p_theta_hat = \n",
    "            bd_learn_linear(Sigma_hat, lambda1, lambda2, vertex_labels, 1, path_to_file,\n",
    "                                    max_iter = max_iter, lr = lr, corr_boost = corr_boost)\n",
    "\n",
    "        thresh = 1e-3\n",
    "        sel_Z = findall(x->abs(x)>thresh, theta_hat)\n",
    "        sel_Z = [x+Z[1]-1 for x in sel_Z]\n",
    "        println(sel_Z)\n",
    "        # test for the null hypothesis\n",
    "        reject_null_hypothesis = \n",
    "          ind_null_hypo(n, num_Z, corr_p_theta_hat; significance_level=(0.01/bonferonni_correction))\n",
    "        # if null rejected, i.e. reject_null_hypothesis=true\n",
    "        if reject_null_hypothesis\n",
    "          # compute ATEs training set\n",
    "          Sigma_wyx_phi = build_phi_covariance(theta_hat, w, y, x, Z, Sigma_hat)\n",
    "        try   \n",
    "          atr_real_train = 0.\n",
    "          ate_hat_train = (Sigma_wyx_phi[[3; 4], [3; 4]] \\ Sigma_wyx_phi[[3; 4], 2])[1]\n",
    "          ate_hat_all_Z_train = (Sigma_hat[[x; Z], [x; Z]] \\ Sigma_hat[[x; Z], y])[1]\n",
    "          ate_hat_best_Z_train = (Sigma_hat[[x; best_Z], [x; best_Z]] \\ Sigma_hat[[x; best_Z], y])[1]\n",
    "          ate_hat_sel_Z_train = (Sigma_hat[[x; sel_Z], [x; sel_Z]] \\ Sigma_hat[[x; sel_Z], y])[1]\n",
    "          ate_hat_marg_Z_train = Sigma_hat[x, y] / Sigma_hat[x, x]\n",
    "\n",
    "          ATE_results_train = [atr_real_train ate_hat_best_Z_train ate_hat_train ate_hat_all_Z_train ate_hat_sel_Z_train ate_hat_marg_Z_train]\n",
    "          # ATE_results_train = [ate_hat_best_Z_train ate_hat_train ate_hat_all_Z_train ate_hat_sel_Z_train ate_hat_marg_Z_train]\n",
    "\n",
    "          # compute \\rho(W,Y|beta*Z, X) validation\n",
    "          Sigma_hat_valid = cov(dat_valid)\n",
    "          Sigma_wyx_phi_valid = build_phi_covariance(theta_hat, w, \n",
    "            x, y, Z, Sigma_hat_valid)\n",
    "   \n",
    "            corr_px_valid = abs(partial_corr([1; 2], [3; 4], Sigma_wyx_phi_valid)[1, 2])\n",
    "            # compute ATEs validation set\n",
    "            atr_real_valid = 0.\n",
    "            ate_hat_valid = (Sigma_wyx_phi_valid[[3; 4], [3; 4]] \\ Sigma_wyx_phi_valid[[3; 4], 2])[1]\n",
    "            ate_hat_all_Z_valid = (Sigma_hat_valid[[x; Z], [x; Z]] \\ Sigma_hat_valid[[x; Z], y])[1]\n",
    "            ate_hat_best_Z_valid = (Sigma_hat_valid[[x; best_Z], [x; best_Z]] \\ Sigma_hat_valid[[x; best_Z], y])[1]\n",
    "            ate_hat_sel_Z_valid = (Sigma_hat_valid[[x; sel_Z], [x; sel_Z]] \\ Sigma_hat_valid[[x; sel_Z], y])[1]\n",
    "            println(ate_hat_sel_Z_valid)\n",
    "            ate_hat_marg_Z_valid = Sigma_hat_valid[x, y] / Sigma_hat_valid[x, x]\n",
    "\n",
    "            ATE_results_valid = [atr_real_valid ate_hat_best_Z_valid ate_hat_valid ate_hat_all_Z_valid ate_hat_sel_Z_valid ate_hat_marg_Z_valid]\n",
    "            # ATE_results_valid = [ate_hat_best_Z_valid ate_hat_valid ate_hat_all_Z_valid ate_hat_sel_Z_valid ate_hat_marg_Z_valid]\n",
    "            # save corrs\n",
    "            hypo_fail[\"seed\"] = seed\n",
    "            hypo_fail[\"corr_px_train\"] = corr_px_theta_hat\n",
    "            hypo_fail[\"corr_p_train\"] = corr_p_theta_hat\n",
    "            hypo_fail[\"corr_px_valid\"] = corr_px_valid\n",
    "            hypo_fail[\"lr\"] = lr\n",
    "            # parameters\n",
    "            hypo_fail[\"theta_hat\"] = theta_hat\n",
    "            hypo_fail[\"sel_Z\"] = sel_Z\n",
    "            hypo_fail[\"lambda1\"] = lambda1\n",
    "            hypo_fail[\"lambda2\"] = lambda2\n",
    "            # ATEs train\n",
    "            hypo_fail[\"ATE_results_train\"] = ATE_results_train\n",
    "            # ATEs valid\n",
    "            hypo_fail[\"ATE_results_valid\"] = ATE_results_valid\n",
    "            hypo_fail_all_lambdas[hypo_fail[\"corr_px_valid\"]] = hypo_fail\n",
    "            break\n",
    "          catch\n",
    "            lambda1 = lambda1 * 2\n",
    "            bonferonni_correction += 1.\n",
    "          end\n",
    "        else\n",
    "          lambda1 = lambda1 * 2\n",
    "          bonferonni_correction += 1.\n",
    "        end\n",
    "      end\n",
    "    end\n",
    "  end\n",
    " end\n",
    "  best_setup = hypo_fail_all_lambdas[minimum(keys(hypo_fail_all_lambdas))]\n",
    "    Sigma_hat_test = cov(dat_test)\n",
    "    Sigma_wyx_phi_test = build_phi_covariance(best_setup[\"theta_hat\"], w, x, y, Z, Sigma_hat_test)\n",
    "    # compute ATEs test set\n",
    "    atr_real_test = 0.\n",
    "    ate_hat_test = (Sigma_wyx_phi_test[[3; 4], [3; 4]] \\ Sigma_wyx_phi_test[[3; 4], 2])[1]\n",
    "    ate_hat_all_Z_test = (Sigma_hat_test[[x; Z], [x; Z]] \\ Sigma_hat_test[[x; Z], y])[1]\n",
    "    ate_hat_best_Z_test = (Sigma_hat_test[[x; best_Z], [x; best_Z]] \\ Sigma_hat_test[[x; best_Z], y])[1]\n",
    "    ate_hat_sel_Z_test = (Sigma_hat_test[[x; best_setup[\"sel_Z\"]], [x; best_setup[\"sel_Z\"]]] \\ Sigma_hat_test[[x; best_setup[\"sel_Z\"]], y])[1]\n",
    "\n",
    "    println(ate_hat_sel_Z_test)\n",
    "    ate_hat_marg_Z_test = Sigma_hat_test[x, y] / Sigma_hat_test[x, x]\n",
    "\n",
    "    ATE_results_test = [atr_real_test ate_hat_best_Z_test ate_hat_test ate_hat_all_Z_test ate_hat_sel_Z_test ate_hat_marg_Z_test]\n",
    "  return best_setup, ATE_results_test\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NHS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset in\n",
    "dat_small_var = zeros(90000, num_vertices);\n",
    "m = 1\n",
    "for row in CSV.Rows(\"../../code/real_world_nhs_dataset/nhs_data_smaller_var.csv\", datarow=2)\n",
    "    dat_small_var[m,:] = [parse(Float64, x) for x in row[2:end]]\n",
    "    m+=1\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute Entner baseline for read dataset\n",
    "split = trunc(Int,(size(dat_small_var)[1]/2))\n",
    "dat_train = dat_small_var[1:split,:]\n",
    "dat_valid = dat_small_var[split+1:end,:];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Installing Julia nodeps kernelspec in /Users/lgultchin/Library/Jupyter/kernels/julia-nodeps-1.2\n",
      "└ @ IJulia /Users/lgultchin/.julia/packages/IJulia/fRegO/deps/kspec.jl:78\n",
      "┌ Info: Loading DataFrames support into Gadfly.jl\n",
      "└ @ Gadfly /Users/lgultchin/.julia/packages/Gadfly/09PWZ/src/mapping.jl:228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ATE_learned"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IJulia.installkernel(\"Julia nodeps\", \"--depwarn=no\")\n",
    "include(\"../../code/real_world_nhs_dataset/simulate.jl\")\n",
    "include(\"../../code/real_world_nhs_dataset/learn_linear.jl\")\n",
    "include(\"../../code/real_world_nhs_dataset/util.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda2: 1.0\n",
      "lr: 5.0e-5\n",
      "[14, 15, 20, 21, 22, 23, 24]\n",
      "[15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
      "-0.1023688604165896\n",
      "lr: 2.0e-5\n",
      "[14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "-0.27396984822314113\n",
      "lr: 0.0005\n",
      "[14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "-0.27396984822314113\n",
      "lambda2: 0.1\n",
      "lr: 5.0e-5\n",
      "[14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "-0.27396984822314113\n",
      "lr: 2.0e-5\n",
      "[14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "-0.27396984822314113\n",
      "lr: 0.0005\n",
      "[14, 16, 17, 19, 21, 22, 23, 24, 25]\n",
      "2.1707132667199054\n",
      "lambda2: 0.01\n",
      "lr: 5.0e-5\n",
      "[14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "9.262140018745267\n",
      "lr: 2.0e-5\n",
      "[14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "-0.27396984822314113\n",
      "lr: 0.0005\n",
      "[14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "-0.27396984822314113\n",
      "lambda2: 1.0\n",
      "lr: 5.0e-5\n",
      "[16, 17, 20, 21, 23, 24]\n",
      "3.6695709510196775\n",
      "lr: 2.0e-5\n",
      "[14, 15, 16, 18, 19, 20, 22, 23, 24, 25]\n",
      "1.8581889336209754\n",
      "lr: 0.0005\n",
      "[14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "[14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "-0.27396984822314113\n",
      "lambda2: 0.1\n",
      "lr: 5.0e-5\n",
      "[14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "-0.2933043501035435\n",
      "lr: 2.0e-5\n",
      "[14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "-0.27396984822314113\n",
      "lr: 0.0005\n",
      "[15, 16, 18, 20, 21, 23, 24]\n",
      "3.6716908097622016\n",
      "lambda2: 0.01\n",
      "lr: 5.0e-5\n",
      "[14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "-0.27396984822314113\n",
      "lr: 2.0e-5\n",
      "[14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "-0.27396984822314113\n",
      "lr: 0.0005\n",
      "[14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "-0.27396984822314113\n",
      "lambda2: 1.0\n",
      "lr: 5.0e-5\n",
      "[15, 17, 20, 21, 24]\n",
      "3.0060749059062446\n",
      "lr: 2.0e-5\n",
      "[14, 15, 16, 18, 19, 22, 23, 25]\n",
      "1.813102885621421\n",
      "lr: 0.0005\n",
      "[14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "-0.27396984822314113\n",
      "lambda2: 0.1\n",
      "lr: 5.0e-5\n",
      "[14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "-0.27396984822314113\n",
      "lr: 2.0e-5\n",
      "[14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "[14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "-0.27396984822314113\n",
      "lr: 0.0005\n",
      "[15, 17, 18, 19, 20, 21, 22, 23, 25]\n",
      "[14, 15, 18, 19, 21, 23, 24, 25]\n",
      "1.5073300622442853\n",
      "lambda2: 0.01\n",
      "lr: 5.0e-5\n",
      "[14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "[14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "-0.27396984822314113\n",
      "lr: 2.0e-5\n",
      "[14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "-0.27396984822314113\n",
      "lr: 0.0005\n",
      "[14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
      "-0.27396984822314113\n",
      "-0.12664746354762166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dict{Any,Any}(\"lr\" => 5.0e-5,\"ATE_results_valid\" => [0.0 0.036316494492162635 … -0.1023688604165896 1.7733204733140853],\"ATE_results_train\" => [0.0 0.03612441828304275 … -0.11833420333802329 1.7815848263285687],\"sel_Z\" => [15, 16, 17, 18, 19, 20, 21, 22, 23],\"lambda2\" => 1.0,\"lambda1\" => 0.001,\"corr_px_valid\" => 0.5912905368409451,\"theta_hat\" => [-3.6727527729830745e-6, 0.545617219575932, 0.05334283112597428, -0.019592604357356536, 0.2898270188110904, -0.08246154060627756, -0.18436372081037034, -0.02816810113084275, -0.042175589063625625, 0.3077244351998149, -6.32423222191002e-5, 5.017706241915715e-5],\"seed\" => 20,\"corr_p_train\" => 0.11575514640347653…), [0.0 0.03647508648596374 … -0.12664746354762166 1.783437366524365])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computereal_world_nhs_dataset/posed algorithm on nhs dataset\n",
    "lambda1=5e-4\n",
    "lambda_twos=[1, 1e-1, 1e-2]\n",
    "lrs=[5e-5, 2e-5, 5e-4]\n",
    "seeds = [20, 40, 60]\n",
    "max_iter = 1000\n",
    "corr_boost=1\n",
    "best_setup, ATEs = choose_lambdas(lambda_twos, lambda1, seeds, dat_small_var, vertex_labels, max_iter, lrs, corr_boost, \"../../code/real_world_nhs_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.0e-5, 1.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_setup[\"lr\"],best_setup[\"lambda2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×6 Array{Float64,2}:\n",
       " 0.0  0.0363165  0.331118  -0.27397  -0.102369  1.77332"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_setup[\"ATE_results_valid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×6 Array{Float64,2}:\n",
       " 0.0  0.0364751  0.333002  -0.287561  -0.126647  1.78344"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are ATEs computed on test set\n",
    "ATEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATEs_results = []\n",
    "append!(ATEs_results, ATEs)\n",
    "# append Entner baseline ATE err\n",
    "append!(ATEs_results, -0.7343839853467186)\n",
    "# This is the pre-computed real ATE, which we will use in the next two cells for ATE_err figures presented in Table 1 in paper\n",
    "ATEs_results[1] = 0.03613994943765849;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are the results in Table 1 of manuscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.163, 0.324, 1.747, 0.771)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(abs(ATEs_results[1] - ATEs_results[5]),digits=3),\n",
    "round(abs(ATEs_results[1] - ATEs_results[4]), digits=3),\n",
    "round(abs(ATEs_results[1] - ATEs_results[6]), digits=3),\n",
    "round(abs(ATEs_results[1] - ATEs_results[7]),digits=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
